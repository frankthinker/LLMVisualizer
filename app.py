"""Streamlit UI for the GPT-2 visual exploration tool."""
from __future__ import annotations

from typing import Dict, Optional

import platform
import subprocess
from pathlib import Path

import streamlit as st
from streamlit.delta_generator import DeltaGenerator
from .gpt2_loader import (
    CACHE_DIR,
    HF_MIRROR_DEFAULT,
    MODEL_SPECS,
    GenerationArtifacts,
    GenerationSettings,
    clear_cached_models,
    describe_model,
    run_generation,
)
from .visualizer import (
    build_attention_figure,
    build_semantic_cluster_figure,
    build_token_dataframe,
    build_token_flow_chart,
    figure_to_html_bytes,
    figure_to_png_bytes,
)

SAMPLE_GROUPS = [
    {
        "key": "science_explain",
        "label": "ç¤ºä¾‹1ï¼šç§‘å­¦è§£è¯»",
        "description": "è‹±æ–‡ç§‘æ™®è¯´æ˜ï¼Œé€‚åˆæŸ¥çœ‹è¯­ä¹‰é“¾è·¯ã€‚",
        "prompts": [
            "Explain the process of photosynthesis to a middle-school student in three clear steps.",
            "How does the water cycle move moisture from warm oceans to snowy mountains? Answer in concise English.",
            "Describe how a solar eclipse happens and why it is brief.",
            "Why do metal objects feel colder than wood even when both are in the same room?",
            "Outline how vaccines train the immune system to recognize viruses.",
            "Explain plate tectonics and how it creates earthquakes at fault lines.",
            "Summarize the greenhouse effect and its role in climate change.",
            "How do bees use vibration and smell to locate flowers?",
            "Describe the difference between potential energy and kinetic energy using a roller coaster example.",
            "Explain why salt lowers the freezing point of water when we melt snow on sidewalks."
        ],
    },
    {
        "key": "story_logic",
        "label": "ç¤ºä¾‹2ï¼šæ•…äº‹æ¨ç†",
        "description": "è‹±æ–‡æ•…äº‹é“¾ï¼Œçªå‡ºæŒ‡ä»£ä¸è¿½è¸ªã€‚",
        "prompts": [
            "A cat chases a mouse, a dog chases the cat, and a boy whistles for the dog. Who controls the chase and why?",
            "Maria hands a key to Ben, Ben shares it with Lila, and Lila returns it to Maria. Who can open the locker last?",
            "Olivia lends her notebook to Kai, Kai forgets it in Maya's bag, and Maya mails it back. Describe the chain of responsibility.",
            "A detective hears three conflicting alibis from siblings. Explain how he can test who is lying.",
            "Grandma bakes pies, leaves one for each grandchild, but two cousins share. Who got the extra slice?",
            "Eli hides a clue under a red chair, Nora moves the chair, and Sam discovers the clue. Who actually solved the puzzle?",
            "Describe how a relay race team depends on each runner not dropping the baton.",
            "A librarian mislabels a book, a student checks it out, and the teacher relies on it. What misunderstanding could happen?",
            "Explain who ultimately owns a painting when it is leased from an artist to a gallery and bought by a collector.",
            "A pilot, a mechanic, and an air-traffic controller share partial information. Show how they cooperate to avoid a delay."
        ],
    },
    {
        "key": "coding_reasoning",
        "label": "ç¤ºä¾‹3ï¼šä»£ç æ¨æ¼”",
        "description": "è‹±æ–‡ä»£ç è§£é‡Šï¼Œå±•ç¤ºæŠ½è±¡æ¨ç†ã€‚",
        "prompts": [
            "Describe step by step how a stack handles the sequence push(3), push(5), pop(), push(7).",
            "Predict what this Python loop prints: total = 0; for n in range(1, 6): total += n; print(total).",
            "Explain what happens when a queue processes enqueue(1), enqueue(4), dequeue(), enqueue(9), dequeue().",
            "In pseudocode, what does a binary search do when the target is smaller than the middle element?",
            "Trace the values of i and sum in: sum=1; for i in range(1,4): sum *= (i+1).",
            "Why does a recursive factorial function need a base case, and what happens without it?",
            "Walk through how a hash map resolves collisions using linear probing.",
            "Explain the time complexity difference between bubble sort and merge sort in simple terms.",
            "Given a Python dictionary comprehension `{k: k*k for k in range(1,5)}`, list the key-value pairs.",
            "Describe how depth-first search explores a tree compared to breadth-first search."
        ],
    },
    {
        "key": "analogy_summary",
        "label": "ç¤ºä¾‹4ï¼šç±»æ¯”æ€»ç»“",
        "description": "è‹±æ–‡ç±»æ¯”æˆ–æ€»ç»“ï¼ŒæŸ¥çœ‹é«˜å±‚è¯­ä¹‰ã€‚",
        "prompts": [
            "Compare teamwork in an ant colony to collaboration inside a human company.",
            "What planning lessons can people learn from the way beavers build dams?",
            "How is a library similar to a well-organized knowledge base inside a computer?",
            "Relate the growth of a city to the way neurons form connections in the brain.",
            "Why is mentoring a new teammate similar to transplanting a seedling into fertile soil?",
            "Explain how a symphony orchestra resembles a cross-functional software team.",
            "Compare a bee colony's decision making to how open-source communities choose priorities.",
            "What can managers learn from the way penguins huddle for warmth in winter?",
            "Relate agile sprints to a relay race where baton handoffs represent knowledge transfer.",
            "How does the ecosystem of a coral reef mirror the dependencies inside a complex product system?",
            "Summarize what human leaders can learn about resilience from migrating birds."
        ],
    },
    {
        "key": "math_reasoning",
        "label": "ç¤ºä¾‹5ï¼šæ•°å­¦æ¨ç†",
        "description": "è‹±æ–‡ç®—æœ¯æ¨ç†é—®é¢˜ï¼Œè§‚å¯Ÿæ¨¡å‹å¯¹æ•°å­—ä¸é€»è¾‘è¯çš„å…³æ³¨ã€‚",
        "prompts": [
            "Mia has 8 apples and gives 2 apples to each of her three friends. How many apples does she have left?",
            "A bakery sold 45 tickets on Friday and twice as many on Saturday. How many tickets were sold during the weekend?",
            "A train travels 120 miles in 3 hours. What is its average speed per hour?",
            "James had 250 dollars, spent 37 on lunch and 45 on books. How much money remains?",
            "A recipe needs 3 cups of flour per batch. How much flour is required for 5 batches?",
            "Lena bikes 15 km to school and the same distance home. How far does she ride in 4 days of classes?",
            "Two numbers add to 48 and differ by 12. What are the two numbers?",
            "A factory produces 1,200 screws a day. How many screws in 6.5 days?",
            "A bookshelf has 5 equally spaced shelves and is 2 meters tall. How far apart are the shelves?",
            "If a car uses 60 liters of fuel to travel 420 km, how many kilometers per liter does it achieve?"
        ],
    },
]


def _init_session_state() -> None:
    """Initialize Streamlit session state keys with defaults."""

    defaults = {
        "prompt_text": SAMPLE_GROUPS[0]["prompts"][0],
        "artifacts": None,
        "last_artifacts": None,
        "selected_token": None,
        "guide_dismissed": False,
        "topk_warned": False,
        "progress_load_pct": 0,
        "progress_load_text": "å‡†å¤‡åŠ è½½ GPT-2 æ¨¡å‹â€¦",
        "progress_infer_pct": 0,
        "progress_infer_text": "ç­‰å¾…æ¨ç†å¼€å§‹â€¦",
        "ui_locked": False,
        "run_pending": False,
        "inference_running": False,
        "queued_settings": None,
        "queued_endpoint": None,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
    for group in SAMPLE_GROUPS:
        idx_key = f"sample_idx_{group['key']}"
        if idx_key not in st.session_state:
            st.session_state[idx_key] = 0


def _show_guide_modal() -> None:
    """Display first-launch usage hints."""

    if st.session_state.get("guide_dismissed", False):
        return

    guide_content = """
    **GPT-2 å¯è§†åŒ–å·¥å…·å¿«é€Ÿå…¥é—¨**

    1. åœ¨å·¦ä¾§è¾“å…¥æ¡†ä¸­è¾“å…¥å¾…åˆ†ææ–‡æœ¬æˆ–åŠ è½½ç¤ºä¾‹ã€‚
    2. å³ä¾§ä¾§è¾¹æ ä¾æ¬¡é€‰æ‹©æ¨¡å‹è§„æ¨¡ã€ç”Ÿæˆé•¿åº¦ä¸é‡‡æ ·å‚æ•°ã€‚
    3. ç‚¹å‡» **ç”Ÿæˆå¹¶å¯è§†åŒ–**ï¼Œè€å¿ƒç­‰å¾…çŠ¶æ€æ åŠ è½½å®Œæˆã€‚
    4. å››ä¸ªæ ‡ç­¾é¡µåˆ†åˆ«å±•ç¤ºè¾“å‡ºæ–‡æœ¬ã€æ³¨æ„åŠ›çƒ­åŠ›å›¾ã€Token æ¨ç†æ—¶åºã€è¯­ä¹‰ç©ºé—´èšç±»ã€‚

    ğŸ“Œ é¼ æ ‡æ‚¬åœå³å¯æŸ¥çœ‹æ•°å€¼è¯¦æƒ…ï¼›ç‚¹å‡»çƒ­åŠ›å›¾ Token å¯è·¨å±‚é«˜äº®ï¼›ä»»æ„å›¾è¡¨éƒ½å¯ä»¥å¯¼å‡ºæˆ HTML æˆ– PNGã€‚
    """

    if hasattr(st, "modal"):
        with st.modal("ä½¿ç”¨æŒ‡å—", key="guide-modal"):
            st.markdown(guide_content)
        if st.button("æˆ‘å·²äº†è§£", type="primary", key="guide-dismiss-modal"):
            st.session_state["guide_dismissed"] = True
    else:
        with st.sidebar.expander("ä½¿ç”¨æŒ‡å—", expanded=True):
            st.markdown(guide_content)
            if st.button("å…³é—­æŒ‡å—", key="guide-dismiss-expander"):
                st.session_state["guide_dismissed"] = True


def _limit_words(text: str, max_words: int = 200) -> str:
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + " â€¦"


def _is_busy() -> bool:
    """Return True when UI should stay disabled (loading or queued)."""

    return bool(
        st.session_state.get("ui_locked", False)
        or st.session_state.get("inference_running", False)
        or st.session_state.get("run_pending", False)
    )


def _open_cache_dir(path: Path) -> None:
    """Open the model cache directory in the system file explorer."""

    try:
        system = platform.system()
        if system == "Darwin":
            subprocess.Popen(["open", str(path)])
        elif system == "Windows":
            subprocess.Popen(["explorer", str(path)])
        else:
            subprocess.Popen(["xdg-open", str(path)])
    except Exception as exc:  # pragma: no cover
        st.warning(f"æ— æ³•æ‰“å¼€æ–‡ä»¶å¤¹ï¼š{exc}")


def _apply_theme_styles(theme_choice: str) -> None:
    """Inject CSS so the entireé¡µé¢è·Ÿéšæ‰€é€‰é…è‰²."""

    if theme_choice == "dark":
        bg_color = "#0b1120"
        card_color = "#111827"
        text_color = "#e5e7eb"
        accent = "#00c2c7"
        button_text = "#3b82f6"
    else:
        bg_color = "#f7f8fb"
        card_color = "#ffffff"
        text_color = "#1f2937"
        accent = "#4757e6"
        button_text = text_color

    st.markdown(
        f"""
        <style>
            .stApp {{
                background-color: {bg_color};
                color: {text_color};
            }}
            .stApp [data-testid="stHeader"] {{
                background: transparent;
            }}
            .stApp div[data-testid="stSidebar"] {{
                background-color: {card_color};
            }}
            .stApp .stTabs [data-baseweb="tab-list"] button[role="tab"] {{
                background-color: {card_color};
                color: {text_color};
            }}
            .stApp .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
                border-bottom: 3px solid {accent};
            }}
            .stApp .stDataFrame, .stApp .stPlotlyChart {{
                background-color: {card_color};
            }}
            .stApp button, .stApp [role="button"] {{
                color: {button_text};
            }}
        </style>
        """,
        unsafe_allow_html=True,
    )


def _render_progress_row() -> Dict[str, DeltaGenerator]:
    """Always display load/inference progress bars using session-backed values."""

    st.markdown("#### åŠ è½½ä¸æ¨ç†è¿›åº¦")
    col_load, col_infer = st.columns(2)
    with col_load:
        st.caption("æ¨¡å‹åŠ è½½çŠ¶æ€")
        load_bar = st.progress(
            st.session_state.get("progress_load_pct", 0),
            text=st.session_state.get("progress_load_text", "å‡†å¤‡åŠ è½½ GPT-2 æ¨¡å‹..."),
        )
    with col_infer:
        st.caption("æ¨ç†æ‰§è¡ŒçŠ¶æ€")
        infer_bar = st.progress(
            st.session_state.get("progress_infer_pct", 0),
            text=st.session_state.get("progress_infer_text", "ç­‰å¾…æ¨ç†å¼€å§‹..."),
        )
    return {"load": load_bar, "infer": infer_bar}


def _render_sidebar() -> Dict[str, object]:
    """Render controls and return selected configuration."""

    is_generating = _is_busy()
    st.sidebar.header("å‚æ•°è°ƒèŠ‚")
    theme_choice = st.sidebar.radio(
        "é…è‰²æ¨¡å¼",
        ["light", "dark"],
        format_func=lambda x: "äº®è‰²" if x == "light" else "æš—è‰²",
        disabled=is_generating,
    )
    model_size = st.sidebar.selectbox(
        "GPT-2 ç‰ˆæœ¬",
        options=list(MODEL_SPECS.keys()),
        format_func=lambda key: MODEL_SPECS[key]["display"],
        index=0,
        disabled=is_generating,
    )
    spec = MODEL_SPECS[model_size]
    model_layers = spec["layers"]
    model_heads = spec["heads"]
    st.sidebar.caption(
        f"{spec['display']} Â· å‚æ•°é‡ {spec.get('params')} Â· å±‚ {spec['layers']} Â· å¤´ {spec['heads']} Â· ä¸Šä¸‹æ–‡ {spec['context']} tokens"
    )
    max_tokens = st.sidebar.slider("ç”Ÿæˆé•¿åº¦", 0, 300, 120, step=5, disabled=is_generating)
    temperature = st.sidebar.slider("æ¸©åº¦", 0.1, 1.0, 0.7, step=0.05, disabled=is_generating)
    top_k = st.sidebar.slider("Top-K", 1, 50, 5, step=1, disabled=is_generating)
    if top_k > 10 and not st.session_state.get("topk_warned"):
        st.sidebar.warning("Top-K è¶…è¿‡ 10 ä¼šæ˜¾è‘—é™ä½å›ç­”å‡†ç¡®åº¦ï¼Œä»…ä¾›ç ”ç©¶ç”¨é€”ã€‚")
        st.session_state["topk_warned"] = True
    attention_layers = st.sidebar.multiselect(
        "æ³¨æ„åŠ›å±‚ (å¯å¤šé€‰)",
        options=list(range(1, model_layers + 1)),
        default=[1, model_layers // 2, model_layers],
        disabled=is_generating,
    )
    attention_heads = st.sidebar.multiselect(
        "æ³¨æ„åŠ›å¤´ (å¯å¤šé€‰)",
        options=list(range(1, model_heads + 1)),
        default=list(range(1, min(12, model_heads) + 1)),
        disabled=is_generating,
    )
    viz_dims = st.sidebar.multiselect(
        "å¯è§†åŒ–ç»´åº¦",
        options=["æ³¨æ„åŠ›æƒé‡", "Token æ¨ç†æµ", "è¯­ä¹‰èšç±»"],
        default=["æ³¨æ„åŠ›æƒé‡", "Token æ¨ç†æµ", "è¯­ä¹‰èšç±»"],
        disabled=is_generating,
    )
    embed_method = st.sidebar.radio(
        "è¯­ä¹‰é™ç»´æ–¹æ³•",
        ["pca", "tsne"],
        format_func=lambda x: x.upper(),
        disabled=is_generating,
    )
    context_limit = st.sidebar.slider(
        "ä¸Šä¸‹æ–‡çª—å£ (tokens)",
        min_value=256,
        max_value=int(spec["context"]),
        value=min(768, int(spec["context"])),
        step=64,
        disabled=is_generating,
    )
    source_choice = st.sidebar.radio(
        "æ¨¡å‹ä¸‹è½½æ¥æº",
        options=["official", "mirror"],
        format_func=lambda key: "Hugging Face å®˜ç½‘" if key == "official" else "é•œåƒç«™ (hf-mirror.com)",
        disabled=is_generating,
    )
    hf_endpoint: Optional[str] = None
    if source_choice == "mirror":
        default_mirror = st.session_state.get("mirror_endpoint", HF_MIRROR_DEFAULT)
        mirror_input = st.sidebar.text_input(
            "é•œåƒåœ°å€",
            value=default_mirror,
            help="ç¤ºä¾‹ï¼šhttps://hf-mirror.com",
            disabled=is_generating,
        )
        resolved_mirror = mirror_input.strip() or HF_MIRROR_DEFAULT
        st.session_state["mirror_endpoint"] = resolved_mirror
        hf_endpoint = resolved_mirror
    else:
        hf_endpoint = None

    with st.sidebar.expander("æ¨¡å‹ç¼“å­˜ä¸æ–‡ä»¶"):
        if st.button("åœ¨èµ„æºç®¡ç†å™¨ä¸­æŸ¥çœ‹æ¨¡å‹ç¼“å­˜", key="open-cache", disabled=is_generating):
            _open_cache_dir(CACHE_DIR)
        if st.button("æ¸…ç†å†…å­˜ä¸­çš„æ¨¡å‹ (é‡Šæ”¾æ˜¾å­˜/RAM)", key="clear-cache", disabled=is_generating):
            clear_cached_models()
            st.toast("å·²æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜ï¼Œä¸‹æ¬¡æ¨ç†ä¼šé‡æ–°åŠ è½½ã€‚")
        st.caption(f"ç¼“å­˜ç›®å½•ï¼š`{CACHE_DIR}`")

    return {
        "theme": theme_choice,
        "settings": GenerationSettings(
            model_size=model_size,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            attention_layers=attention_layers,
            attention_heads=attention_heads,
            max_context_tokens=context_limit,
        ),
        "viz_dims": viz_dims,
        "embed_method": embed_method,
        "hf_endpoint": hf_endpoint,
    }


def _handle_actions(config: Dict[str, object]) -> None:
    """Handle generation, clearing, and export triggers."""

    artifacts = st.session_state.get("artifacts")
    report_bytes = (
        _build_report_html(artifacts, config).encode("utf-8") if artifacts else b""
    )

    is_generating = _is_busy()
    col_run, col_clear, col_export = st.columns([2, 1, 1])
    run_clicked = col_run.button(
        "ç”Ÿæˆå¹¶å¯è§†åŒ–",
        type="primary",
        width="stretch",
        disabled=is_generating,
    )
    clear_clicked = col_clear.button(
        "æ¸…ç©ºæ‰€æœ‰ç»“æœ", width="stretch", disabled=is_generating or artifacts is None
    )
    col_export.download_button(
        "å¯¼å‡ºæ±‡æ€» (HTML)",
        data=report_bytes,
        file_name="gpt2_visual_report.html",
        mime="text/html",
        width="stretch",
        disabled=is_generating or artifacts is None,
    )

    if clear_clicked:
        st.session_state["artifacts"] = None
        st.session_state["last_artifacts"] = None
        st.session_state["selected_token"] = None
        st.session_state["prompt_text"] = ""
        st.toast("å·²æ¸…ç©ºå†å²ç»“æœã€‚")

    if run_clicked and not is_generating:
        st.session_state["queued_settings"] = config["settings"]
        st.session_state["queued_endpoint"] = config["hf_endpoint"]
        # åŒæ­¥åŠ¨ä½œï¼šç«‹åˆ»é”å®šå…¨éƒ¨æŒ‰é’®ï¼Œå¹¶æ’é˜Ÿå¯åŠ¨æ¨ç†ï¼›rerun è§¦å‘åæ¨¡å‹ä¸‹è½½é©¬ä¸Šå¼€å§‹
        st.session_state["ui_locked"] = True
        st.session_state["run_pending"] = True
        st.rerun()


def _build_report_html(artifacts: GenerationArtifacts, config: Dict[str, object]) -> str:
    """Compose a lightweight HTML report summarizing key outputs."""

    text_section = f"""<h2>åŸå§‹è¾“å‡º</h2><p>{artifacts.generated_text}</p>"""
    endpoint_label = config.get("hf_endpoint") or "huggingface.co"
    meta_section = (
        f"<p>æ¨¡å‹: {describe_model(artifacts.model_size)} / æ¸©åº¦ {config['settings'].temperature} / "
        f"Top-K {config['settings'].top_k} / ä¸‹è½½æº {endpoint_label}</p>"
    )
    tables = build_token_dataframe(artifacts).to_html(index=False)
    return f"<html><body>{meta_section}{text_section}<h3>Token æ˜ç»†</h3>{tables}</body></html>"


def _run_inference(settings: GenerationSettings, hf_endpoint: Optional[str]) -> None:
    """Execute GPT-2 generation with progress feedback."""

    prompt = st.session_state.get("prompt_text", "").strip()
    if not prompt:
        st.warning("è¯·è¾“å…¥æˆ–åŠ è½½ä¸€æ®µæ–‡æœ¬ã€‚")
        st.session_state["run_pending"] = False
        st.session_state["inference_running"] = False
        st.session_state["ui_locked"] = False
        return

    progress_widgets = st.session_state.get("progress_widgets")
    if not progress_widgets:
        progress_widgets = _render_progress_row()
        st.session_state["progress_widgets"] = progress_widgets
    load_bar = progress_widgets["load"]
    infer_bar = progress_widgets["infer"]

    def safe_progress(bar, percent: int, text: str) -> None:
        pct = max(0, min(100, int(percent)))
        try:
            bar.progress(pct, text=text)
        except Exception:
            pass

    def progress_callback(stage: str, percent: int) -> None:
        if stage == "download":
            safe_progress(load_bar, percent, f"æ¨¡å‹åŠ è½½ {percent}%")
            st.session_state["progress_load_pct"] = percent
            st.session_state["progress_load_text"] = f"æ¨¡å‹åŠ è½½ {percent}%"
        elif stage == "inference":
            safe_progress(infer_bar, percent, f"æ¨ç†è¿›åº¦ {percent}%")
            st.session_state["progress_infer_pct"] = percent
            st.session_state["progress_infer_text"] = f"æ¨ç†è¿›åº¦ {percent}%"

    try:
        artifacts = run_generation(
            prompt,
            settings,
            hf_endpoint=hf_endpoint,
            progress_callback=progress_callback,
        )
        safe_progress(infer_bar, 100, "æ¨ç†è¿›åº¦ 100%")
        st.session_state["progress_infer_pct"] = 100
        st.session_state["progress_infer_text"] = "æ¨ç†è¿›åº¦ 100%"
        st.session_state["artifacts"] = artifacts
        st.session_state["last_artifacts"] = artifacts
        st.session_state["selected_token"] = None
        safe_progress(load_bar, 100, "æ¨¡å‹åŠ è½½å®Œæˆ")
        safe_progress(infer_bar, 100, "æ¨ç†å®Œæˆ")
        st.session_state["progress_load_pct"] = 100
        st.session_state["progress_load_text"] = "æ¨¡å‹åŠ è½½å®Œæˆ"
        st.session_state["progress_infer_pct"] = 100
        st.session_state["progress_infer_text"] = "æ¨ç†å®Œæˆ"
        st.success("ç”Ÿæˆå’Œç‰¹å¾æŠ½å–å®Œæˆã€‚")
    except Exception as exc:  # pylint: disable=broad-except
        st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{exc}")
    finally:
        try:
            load_bar.empty()
            infer_bar.empty()
        except Exception:
            pass


def _render_prompt_area() -> None:
    """Render text input and sample chips."""

    st.subheader("è¾“å…¥åŒº")
    st.caption("è¾“å…¥ä»»æ„æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨ä¸‹æ–¹ç¤ºä¾‹æŒ‰é’®ã€‚âš ï¸ GPT-2 ä»¥è‹±æ–‡è¯­æ–™ä¸ºä¸»ï¼Œå»ºè®®ä¼˜å…ˆè¾“å…¥è‹±æ–‡å†…å®¹ã€‚")

    is_generating = _is_busy()
    button_cols = st.columns(len(SAMPLE_GROUPS))
    for col, group in zip(button_cols, SAMPLE_GROUPS):
        if col.button(
            group["label"],
            key=f"sample-btn-{group['key']}",
            help=group["description"],
            width="stretch",
            disabled=is_generating,
        ):
            idx_key = f"sample_idx_{group['key']}"
            current_idx = st.session_state.get(idx_key, 0)
            prompt = group["prompts"][current_idx]
            st.session_state["prompt_text"] = prompt
            st.session_state[idx_key] = (current_idx + 1) % len(group["prompts"])
            st.toast(f"å·²ç»è½½å…¥ {group['label']} Â· ç¤ºä¾‹ {current_idx + 1}")
    st.session_state["prompt_text"] = st.text_area(
        "å¾…åˆ†ææ–‡æœ¬",
        value=st.session_state["prompt_text"],
        height=150,
        placeholder="ä¾‹å¦‚ï¼šExplain how light turns into energy inside a plant leaf.",
    )
    st.caption("Tip: Use concise English prompts to obtain more stable attention and semantic visualizations.")


def _render_results(config: Dict[str, object]) -> None:
    """Show visualization tabs based on generated artifacts."""

    artifacts: Optional[GenerationArtifacts] = st.session_state.get("artifacts")
    if artifacts is None:
        artifacts = st.session_state.get("last_artifacts")
    if not artifacts:
        return

    tabs = st.tabs(["åŸå§‹è¾“å‡º", "æ³¨æ„åŠ›å¯è§†åŒ–", "Token æ¨ç†æ—¶åºæµ", "è¯­ä¹‰ç©ºé—´èšç±»"])

    with tabs[0]:
        st.markdown("### åŸå§‹è¾“å‡º (å« Token)")
        st.code(_limit_words(artifacts.generated_text or "(æ¨¡å‹ç”Ÿæˆä¸ºç©º)", 200))
        token_df = build_token_dataframe(artifacts)
        st.dataframe(token_df, width="stretch", hide_index=True)
        st.caption("æç¤ºï¼šæ¦‚ç‡è¶Šé«˜çš„ Token è¶Šç¡®å®šï¼Œå›°æƒ‘åº¦å±•ç¤ºæ¨¡å‹ä¸ç¡®å®šçš„èŠ‚ç‚¹ã€‚")

    with tabs[1]:
        if "æ³¨æ„åŠ›æƒé‡" in config["viz_dims"]:
            st.markdown("### æ³¨æ„åŠ›å±‚çº§")
            available_layers = config["settings"].attention_layers
            visible_layers = available_layers
            if len(available_layers) > 3:
                max_start = len(available_layers) - 3
                start_idx = st.slider(
                    "é€‰æ‹©æ³¨æ„åŠ›å±‚çª—å£ï¼ˆä¸€æ¬¡æœ€å¤šå±•ç¤º 3 å±‚ï¼‰",
                    min_value=0,
                    max_value=max_start,
                    value=st.session_state.get("attention_layer_window", 0),
                    key="attention-layer-window-slider",
                )
                st.session_state["attention_layer_window"] = start_idx
                visible_layers = available_layers[start_idx : start_idx + 3]
                st.caption(f"å½“å‰å±•ç¤ºå±‚ï¼š{visible_layers} Â· å…±é€‰æ‹© {len(available_layers)} å±‚ï¼Œå¯æ‹–åŠ¨æ»‘å—åˆ‡æ¢ã€‚")
            else:
                st.caption(f"å½“å‰å±•ç¤ºå±‚ï¼š{visible_layers}")

            token_count = len(artifacts.tokens)
            highlight_default = min(st.session_state.get("selected_token") or 0, max(token_count - 1, 0))
            if token_count > 1:
                highlight_idx = st.slider(
                    "é€‰æ‹©é«˜äº® Token ç´¢å¼•",
                    min_value=0,
                    max_value=token_count - 1,
                    value=highlight_default,
                    key="token-highlight-slider",
                )
                st.session_state["selected_token"] = highlight_idx
                st.caption(f"å½“å‰ Token #{highlight_idx}: `{artifacts.tokens[highlight_idx]}`")
            else:
                st.session_state["selected_token"] = None
            attention_fig, summaries = build_attention_figure(
                artifacts,
                layers=visible_layers,
                heads=config["settings"].attention_heads,
                theme=config["theme"],
                selected_token=st.session_state.get("selected_token"),
            )
            st.plotly_chart(attention_fig, width="stretch")
            st.markdown("\n".join([f"- {summary}" for summary in summaries]))
            _render_download_row(attention_fig, prefix="attention")
            st.caption("åº•å±‚å±‚å…³æ³¨è¯­æ³•é‚»è¿‘ï¼Œé«˜å±‚å±‚èšç„¦æŠ½è±¡è¯­ä¹‰ã€‚é€šè¿‡ä¸Šæ–¹æ»‘å—æŒ‘é€‰ Tokenï¼Œå¯è·¨å±‚è¿½è¸ªå…¶æ³¨æ„åŠ›ã€‚")
        else:
            st.warning("å·²å…³é—­æ³¨æ„åŠ›è§†å›¾ï¼Œå¯åœ¨ä¾§è¾¹æ é‡æ–°å¯ç”¨ã€‚")

    with tabs[2]:
        if "Token æ¨ç†æµ" in config["viz_dims"]:
            st.markdown("### æ¨ç†æµç¨‹")
            flow_fig, flow_df = build_token_flow_chart(artifacts, theme=config["theme"])
            st.plotly_chart(flow_fig, width="stretch")
            st.dataframe(flow_df, width="stretch", hide_index=True)
            _render_download_row(flow_fig, prefix="token_flow")
            st.caption("çº¢è‰²æ ‡è®°è¡¨ç¤ºæ¨¡å‹é«˜åº¦è‡ªä¿¡ï¼Œé»„è‰²æ„å‘³ç€çŠ¹è±«èŠ‚ç‚¹ï¼Œå¯ç”¨æ¥è§£é‡Šç”ŸæˆèŠ‚å¥ã€‚")
        else:
            st.warning("å·²å…³é—­ Token æ¨ç†æµè§†å›¾ã€‚")

    with tabs[3]:
        if "è¯­ä¹‰èšç±»" in config["viz_dims"]:
            st.markdown("### è¯­ä¹‰èšç±»")
            cluster_fig, cluster_df = build_semantic_cluster_figure(
                artifacts,
                layers=config["settings"].attention_layers,
                method=config["embed_method"],
                theme=config["theme"],
            )
            st.plotly_chart(cluster_fig, width="stretch")
            st.dataframe(cluster_df.head(100), width="stretch", hide_index=True)
            _render_download_row(cluster_fig, prefix="semantic")
            st.caption("æ•£ç‚¹ä½ç½®å±•ç¤º Token åœ¨è¯­ä¹‰ç©ºé—´çš„æŠ•å½±ï¼Œç›¸åŒé¢œè‰²è¡¨ç¤ºè¯­ä¹‰ç±»åˆ«ã€‚")
        else:
            st.warning("å·²å…³é—­è¯­ä¹‰èšç±»è§†å›¾ã€‚")


def _render_download_row(fig, prefix: str) -> None:
    """Render HTML and PNG download buttons for a figure."""

    col_html, col_png = st.columns(2)
    with col_html:
        st.download_button(
            "å¯¼å‡º HTML",
            data=figure_to_html_bytes(fig),
            file_name=f"{prefix}.html",
            mime="text/html",
            width="stretch",
        )
    with col_png:
        st.download_button(
            "å¯¼å‡º PNG",
            data=figure_to_png_bytes(fig),
            file_name=f"{prefix}.png",
            mime="image/png",
            width="stretch",
        )


def main() -> None:
    """Entrypoint for Streamlit."""

    st.set_page_config(page_title="GPT-2 å¯è§†åŒ–å·¥ä½œå°", page_icon="ğŸ§ ", layout="wide")
    _init_session_state()
    st.session_state["progress_widgets"] = _render_progress_row()
    if st.session_state.get("run_pending"):
        st.session_state["ui_locked"] = True
        st.session_state["inference_running"] = True
        st.session_state["queued_settings"] = st.session_state.get("queued_settings")
        st.session_state["queued_endpoint"] = st.session_state.get("queued_endpoint")
        st.session_state["run_pending"] = False
        st.session_state["inference_triggered"] = True
        st.rerun()

    if st.session_state.get("inference_triggered"):
        st.session_state["inference_triggered"] = False
        settings = st.session_state.pop("queued_settings", None)
        endpoint = st.session_state.pop("queued_endpoint", None)
        _run_inference(settings, endpoint)
        st.session_state["inference_running"] = False
        st.session_state["ui_locked"] = False
    _show_guide_modal()
    config = _render_sidebar()
    _apply_theme_styles(config["theme"])
    st.title("GPT-2 å¯è§†åŒ–å·¥ä½œå°")
    st.caption("é¢å‘éä¸“ä¸šç”¨æˆ·çš„å±‚çº§æ€ç»´é€è§†â€”â€”åœ¨ CPU ä¸Šä¹Ÿèƒ½è¿è¡Œçš„å¯è§†åŒ–å·¥å…·ã€‚")
    st.markdown(f"å½“å‰æ¨¡å‹ï¼š{describe_model(config['settings'].model_size)}")
    _render_prompt_area()
    _handle_actions(config)
    st.divider()
    _render_results(config)


if __name__ == "__main__":
    main()
