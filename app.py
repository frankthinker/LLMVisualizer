"""Streamlit UI for the GPT-2 visual exploration tool."""
from __future__ import annotations

from typing import Dict, Optional

import platform
import subprocess
from pathlib import Path

import streamlit as st
from .gpt2_loader import (
    CACHE_DIR,
    HF_MIRROR_DEFAULT,
    MODEL_SPECS,
    GenerationArtifacts,
    GenerationSettings,
    clear_cached_models,
    describe_model,
    run_generation,
)
from .visualizer import (
    build_attention_figure,
    build_semantic_cluster_figure,
    build_token_dataframe,
    build_token_flow_chart,
    figure_to_html_bytes,
    figure_to_png_bytes,
)

SAMPLE_GROUPS = [
    {
        "key": "math_reasoning",
        "label": "ç¤ºä¾‹1ï¼šæ•°å­¦æ¨ç†",
        "description": "è‹±æ–‡ç®—æœ¯æ¨ç†é—®é¢˜ï¼Œè§‚å¯Ÿæ¨¡å‹å¯¹æ•°å­—ä¸é€»è¾‘è¯çš„å…³æ³¨ã€‚",
        "prompts": [
            "Mia has 8 apples and gives 2 apples to each of her three friends. How many apples does she have left?",
            "A bakery sold 45 tickets on Friday and twice as many on Saturday. How many tickets were sold during the weekend?"
        ],
    },
    {
        "key": "science_explain",
        "label": "ç¤ºä¾‹2ï¼šç§‘å­¦è§£è¯»",
        "description": "è‹±æ–‡ç§‘æ™®è¯´æ˜ï¼Œé€‚åˆæŸ¥çœ‹è¯­ä¹‰é“¾è·¯ã€‚",
        "prompts": [
            "Explain the process of photosynthesis to a middle-school student in three clear steps.",
            "How does the water cycle move moisture from warm oceans to snowy mountains? Answer in concise English."
        ],
    },
    {
        "key": "story_logic",
        "label": "ç¤ºä¾‹3ï¼šæ•…äº‹æ¨ç†",
        "description": "è‹±æ–‡æ•…äº‹é“¾ï¼Œçªå‡ºæŒ‡ä»£ä¸è¿½è¸ªã€‚",
        "prompts": [
            "A cat chases a mouse, a dog chases the cat, and a boy whistles for the dog. Who controls the chase and why?",
            "Maria hands a key to Ben, Ben shares it with Lila, and Lila returns it to Maria. Who can open the locker last?"
        ],
    },
    {
        "key": "coding_reasoning",
        "label": "ç¤ºä¾‹4ï¼šä»£ç æ¨æ¼”",
        "description": "è‹±æ–‡ä»£ç è§£é‡Šï¼Œå±•ç¤ºæŠ½è±¡æ¨ç†ã€‚",
        "prompts": [
            "Describe step by step how a stack handles the sequence push(3), push(5), pop(), push(7).",
            "Predict what this Python loop prints: total = 0; for n in range(1, 6): total += n; print(total)."
        ],
    },
    {
        "key": "analogy_summary",
        "label": "ç¤ºä¾‹5ï¼šç±»æ¯”æ€»ç»“",
        "description": "è‹±æ–‡ç±»æ¯”æˆ–æ€»ç»“ï¼ŒæŸ¥çœ‹é«˜å±‚è¯­ä¹‰ã€‚",
        "prompts": [
            "Compare teamwork in an ant colony to collaboration inside a human company.",
            "What planning lessons can people learn from the way beavers build dams?"
        ],
    },
]


def _init_session_state() -> None:
    """Initialize Streamlit session state keys with defaults."""

    defaults = {
        "prompt_text": SAMPLE_GROUPS[0]["prompts"][0],
        "artifacts": None,
        "selected_token": None,
        "guide_dismissed": False,
        "topk_warned": False,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
    for group in SAMPLE_GROUPS:
        idx_key = f"sample_idx_{group['key']}"
        if idx_key not in st.session_state:
            st.session_state[idx_key] = 0


def _show_guide_modal() -> None:
    """Display first-launch usage hints."""

    if st.session_state.get("guide_dismissed", False):
        return

    guide_content = """
    **GPT-2 å¯è§†åŒ–å·¥å…·å¿«é€Ÿå…¥é—¨**

    1. åœ¨å·¦ä¾§è¾“å…¥æ¡†ä¸­è¾“å…¥å¾…åˆ†ææ–‡æœ¬æˆ–åŠ è½½ç¤ºä¾‹ã€‚
    2. å³ä¾§ä¾§è¾¹æ ä¾æ¬¡é€‰æ‹©æ¨¡å‹è§„æ¨¡ã€ç”Ÿæˆé•¿åº¦ä¸é‡‡æ ·å‚æ•°ã€‚
    3. ç‚¹å‡» **ç”Ÿæˆå¹¶å¯è§†åŒ–**ï¼Œè€å¿ƒç­‰å¾…çŠ¶æ€æ åŠ è½½å®Œæˆã€‚
    4. å››ä¸ªæ ‡ç­¾é¡µåˆ†åˆ«å±•ç¤ºè¾“å‡ºæ–‡æœ¬ã€æ³¨æ„åŠ›çƒ­åŠ›å›¾ã€Token æ¨ç†æ—¶åºã€è¯­ä¹‰ç©ºé—´èšç±»ã€‚

    ğŸ“Œ é¼ æ ‡æ‚¬åœå³å¯æŸ¥çœ‹æ•°å€¼è¯¦æƒ…ï¼›ç‚¹å‡»çƒ­åŠ›å›¾ Token å¯è·¨å±‚é«˜äº®ï¼›ä»»æ„å›¾è¡¨éƒ½å¯ä»¥å¯¼å‡ºæˆ HTML æˆ– PNGã€‚
    """

    if hasattr(st, "modal"):
        with st.modal("ä½¿ç”¨æŒ‡å—", key="guide-modal"):
            st.markdown(guide_content)
        if st.button("æˆ‘å·²äº†è§£", type="primary", key="guide-dismiss-modal"):
            st.session_state["guide_dismissed"] = True
    else:
        with st.sidebar.expander("ä½¿ç”¨æŒ‡å—", expanded=True):
            st.markdown(guide_content)
            if st.button("å…³é—­æŒ‡å—", key="guide-dismiss-expander"):
                st.session_state["guide_dismissed"] = True


def _limit_words(text: str, max_words: int = 200) -> str:
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + " â€¦"


def _open_cache_dir(path: Path) -> None:
    """Open the model cache directory in the system file explorer."""

    try:
        system = platform.system()
        if system == "Darwin":
            subprocess.Popen(["open", str(path)])
        elif system == "Windows":
            subprocess.Popen(["explorer", str(path)])
        else:
            subprocess.Popen(["xdg-open", str(path)])
    except Exception as exc:  # pragma: no cover
        st.warning(f"æ— æ³•æ‰“å¼€æ–‡ä»¶å¤¹ï¼š{exc}")


def _apply_theme_styles(theme_choice: str) -> None:
    """Inject CSS so the entireé¡µé¢è·Ÿéšæ‰€é€‰é…è‰²."""

    if theme_choice == "dark":
        bg_color = "#0b1120"
        card_color = "#111827"
        text_color = "#e5e7eb"
        accent = "#00c2c7"
    else:
        bg_color = "#f7f8fb"
        card_color = "#ffffff"
        text_color = "#1f2937"
        accent = "#4757e6"

    st.markdown(
        f"""
        <style>
            .stApp {{
                background-color: {bg_color};
                color: {text_color};
            }}
            .stApp [data-testid="stHeader"] {{
                background: transparent;
            }}
            .stApp div[data-testid="stSidebar"] {{
                background-color: {card_color};
            }}
            .stApp .stTabs [data-baseweb="tab-list"] button[role="tab"] {{
                background-color: {card_color};
                color: {text_color};
            }}
            .stApp .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
                border-bottom: 3px solid {accent};
            }}
            .stApp .stDataFrame, .stApp .stPlotlyChart {{
                background-color: {card_color};
            }}
        </style>
        """,
        unsafe_allow_html=True,
    )


def _render_sidebar() -> Dict[str, object]:
    """Render controls and return selected configuration."""

    st.sidebar.header("å‚æ•°è°ƒèŠ‚")
    theme_choice = st.sidebar.radio("é…è‰²æ¨¡å¼", ["light", "dark"], format_func=lambda x: "äº®è‰²" if x == "light" else "æš—è‰²")
    model_size = st.sidebar.selectbox(
        "GPT-2 ç‰ˆæœ¬",
        options=list(MODEL_SPECS.keys()),
        format_func=lambda key: MODEL_SPECS[key]["display"],
        index=0,
    )
    spec = MODEL_SPECS[model_size]
    model_layers = spec["layers"]
    model_heads = spec["heads"]
    st.sidebar.caption(
        f"{spec['display']} Â· å‚æ•°é‡ {spec.get('params')} Â· å±‚ {spec['layers']} Â· å¤´ {spec['heads']} Â· ä¸Šä¸‹æ–‡ {spec['context']} tokens"
    )
    max_tokens = st.sidebar.slider("ç”Ÿæˆé•¿åº¦", 0, 300, 120, step=5)
    temperature = st.sidebar.slider("æ¸©åº¦", 0.1, 1.0, 0.7, step=0.05)
    top_k = st.sidebar.slider("Top-K", 1, 50, 5, step=1)
    if top_k > 10 and not st.session_state.get("topk_warned"):
        st.sidebar.warning("Top-K è¶…è¿‡ 10 ä¼šæ˜¾è‘—é™ä½å›ç­”å‡†ç¡®åº¦ï¼Œä»…ä¾›ç ”ç©¶ç”¨é€”ã€‚")
        st.session_state["topk_warned"] = True
    attention_layers = st.sidebar.multiselect(
        "æ³¨æ„åŠ›å±‚ (å¯å¤šé€‰)",
        options=list(range(1, model_layers + 1)),
        default=[1, model_layers // 2, model_layers],
    )
    attention_heads = st.sidebar.multiselect(
        "æ³¨æ„åŠ›å¤´ (å¯å¤šé€‰)",
        options=list(range(1, model_heads + 1)),
        default=list(range(1, min(12, model_heads) + 1)),
    )
    viz_dims = st.sidebar.multiselect(
        "å¯è§†åŒ–ç»´åº¦",
        options=["æ³¨æ„åŠ›æƒé‡", "Token æ¨ç†æµ", "è¯­ä¹‰èšç±»"],
        default=["æ³¨æ„åŠ›æƒé‡", "Token æ¨ç†æµ", "è¯­ä¹‰èšç±»"],
    )
    embed_method = st.sidebar.radio("è¯­ä¹‰é™ç»´æ–¹æ³•", ["pca", "tsne"], format_func=lambda x: x.upper())
    context_limit = st.sidebar.slider(
        "ä¸Šä¸‹æ–‡çª—å£ (tokens)",
        min_value=256,
        max_value=int(spec["context"]),
        value=min(768, int(spec["context"])),
        step=64,
    )
    source_choice = st.sidebar.radio(
        "æ¨¡å‹ä¸‹è½½æ¥æº",
        options=["official", "mirror"],
        format_func=lambda key: "Hugging Face å®˜ç½‘" if key == "official" else "é•œåƒç«™ (hf-mirror.com)",
    )
    hf_endpoint: Optional[str] = None
    if source_choice == "mirror":
        default_mirror = st.session_state.get("mirror_endpoint", HF_MIRROR_DEFAULT)
        mirror_input = st.sidebar.text_input("é•œåƒåœ°å€", value=default_mirror, help="ç¤ºä¾‹ï¼šhttps://hf-mirror.com")
        resolved_mirror = mirror_input.strip() or HF_MIRROR_DEFAULT
        st.session_state["mirror_endpoint"] = resolved_mirror
        hf_endpoint = resolved_mirror
    else:
        hf_endpoint = None

    with st.sidebar.expander("æ¨¡å‹ç¼“å­˜ä¸æ–‡ä»¶"):
        if st.button("åœ¨èµ„æºç®¡ç†å™¨ä¸­æŸ¥çœ‹æ¨¡å‹ç¼“å­˜", key="open-cache"):
            _open_cache_dir(CACHE_DIR)
        if st.button("æ¸…ç†å†…å­˜ä¸­çš„æ¨¡å‹ (é‡Šæ”¾æ˜¾å­˜/RAM)", key="clear-cache"):
            clear_cached_models()
            st.toast("å·²æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜ï¼Œä¸‹æ¬¡æ¨ç†ä¼šé‡æ–°åŠ è½½ã€‚")
        st.caption(f"ç¼“å­˜ç›®å½•ï¼š`{CACHE_DIR}`")

    return {
        "theme": theme_choice,
        "settings": GenerationSettings(
            model_size=model_size,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            attention_layers=attention_layers,
            attention_heads=attention_heads,
            max_context_tokens=context_limit,
        ),
        "viz_dims": viz_dims,
        "embed_method": embed_method,
        "hf_endpoint": hf_endpoint,
    }


def _handle_actions(config: Dict[str, object]) -> None:
    """Handle generation, clearing, and export triggers."""

    artifacts = st.session_state.get("artifacts")
    report_bytes = (
        _build_report_html(artifacts, config).encode("utf-8") if artifacts else b""
    )

    col_run, col_clear, col_export = st.columns([2, 1, 1])
    run_clicked = col_run.button("ç”Ÿæˆå¹¶å¯è§†åŒ–", type="primary", width="stretch")
    clear_clicked = col_clear.button("æ¸…ç©ºæ‰€æœ‰ç»“æœ", width="stretch", disabled=artifacts is None)
    col_export.download_button(
        "å¯¼å‡ºæ±‡æ€» (HTML)",
        data=report_bytes,
        file_name="gpt2_visual_report.html",
        mime="text/html",
        width="stretch",
        disabled=artifacts is None,
    )

    if clear_clicked:
        st.session_state["artifacts"] = None
        st.session_state["selected_token"] = None
        st.session_state["prompt_text"] = ""
        st.toast("å·²æ¸…ç©ºå†å²ç»“æœã€‚")

    if run_clicked:
        _run_inference(config["settings"], config["hf_endpoint"])


def _build_report_html(artifacts: GenerationArtifacts, config: Dict[str, object]) -> str:
    """Compose a lightweight HTML report summarizing key outputs."""

    text_section = f"""<h2>åŸå§‹è¾“å‡º</h2><p>{artifacts.generated_text}</p>"""
    endpoint_label = config.get("hf_endpoint") or "huggingface.co"
    meta_section = (
        f"<p>æ¨¡å‹: {describe_model(artifacts.model_size)} / æ¸©åº¦ {config['settings'].temperature} / "
        f"Top-K {config['settings'].top_k} / ä¸‹è½½æº {endpoint_label}</p>"
    )
    tables = build_token_dataframe(artifacts).to_html(index=False)
    return f"<html><body>{meta_section}{text_section}<h3>Token æ˜ç»†</h3>{tables}</body></html>"


def _run_inference(settings: GenerationSettings, hf_endpoint: Optional[str]) -> None:
    """Execute GPT-2 generation with progress feedback."""

    prompt = st.session_state.get("prompt_text", "").strip()
    if not prompt:
        st.warning("è¯·è¾“å…¥æˆ–åŠ è½½ä¸€æ®µæ–‡æœ¬ã€‚")
        return

    load_bar = st.progress(0, text="å‡†å¤‡åŠ è½½ GPT-2 æ¨¡å‹...")
    infer_bar = st.progress(0, text="ç­‰å¾…æ¨ç†å¼€å§‹...")

    def progress_callback(stage: str, percent: int) -> None:
        pct = max(0, min(100, int(percent)))
        if stage == "download":
            load_bar.progress(pct, text=f"æ¨¡å‹åŠ è½½ {pct}%")
        elif stage == "inference":
            infer_bar.progress(pct, text=f"æ¨ç†è¿›åº¦ {pct}%")

    try:
        artifacts = run_generation(
            prompt,
            settings,
            hf_endpoint=hf_endpoint,
            progress_callback=progress_callback,
        )
        infer_bar.progress(100, text="æ¨ç†è¿›åº¦ 100%")
        st.session_state["artifacts"] = artifacts
        st.session_state["selected_token"] = None
        load_bar.progress(100, text="æ¨¡å‹åŠ è½½å®Œæˆ")
        infer_bar.progress(100, text="æ¨ç†å®Œæˆ")
        st.success("ç”Ÿæˆå’Œç‰¹å¾æŠ½å–å®Œæˆã€‚")
    except Exception as exc:  # pylint: disable=broad-except
        st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{exc}")
    finally:
        load_bar.empty()
        infer_bar.empty()


def _render_prompt_area() -> None:
    """Render text input and sample chips."""

    st.subheader("è¾“å…¥åŒº")
    st.caption("è¾“å…¥ä»»æ„æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨ä¸‹æ–¹ç¤ºä¾‹æŒ‰é’®ã€‚âš ï¸ GPT-2 ä»¥è‹±æ–‡è¯­æ–™ä¸ºä¸»ï¼Œå»ºè®®ä¼˜å…ˆè¾“å…¥è‹±æ–‡å†…å®¹ã€‚")

    button_cols = st.columns(len(SAMPLE_GROUPS))
    for col, group in zip(button_cols, SAMPLE_GROUPS):
        if col.button(
            group["label"],
            key=f"sample-btn-{group['key']}",
            help=group["description"],
            width="stretch",
        ):
            idx_key = f"sample_idx_{group['key']}"
            current_idx = st.session_state.get(idx_key, 0)
            prompt = group["prompts"][current_idx]
            st.session_state["prompt_text"] = prompt
            st.session_state[idx_key] = (current_idx + 1) % len(group["prompts"])
            st.toast(f"å·²ç»è½½å…¥ {group['label']} Â· ç¤ºä¾‹ {current_idx + 1}")

    st.session_state["prompt_text"] = st.text_area(
        "å¾…åˆ†ææ–‡æœ¬",
        value=st.session_state["prompt_text"],
        height=150,
        placeholder="ä¾‹å¦‚ï¼šExplain how light turns into energy inside a plant leaf.",
    )
    st.caption("Tip: Use concise English prompts to obtain more stable attention and semantic visualizations.")


def _render_results(config: Dict[str, object]) -> None:
    """Show visualization tabs based on generated artifacts."""

    artifacts: Optional[GenerationArtifacts] = st.session_state.get("artifacts")
    if not artifacts:
        st.info("ç­‰å¾…ç”Ÿæˆç»“æœåå°†æ˜¾ç¤ºå¯è§†åŒ–å†…å®¹ã€‚")
        return

    tabs = st.tabs(["åŸå§‹è¾“å‡º", "æ³¨æ„åŠ›å¯è§†åŒ–", "Token æ¨ç†æ—¶åºæµ", "è¯­ä¹‰ç©ºé—´èšç±»"])

    with tabs[0]:
        st.markdown("### åŸå§‹è¾“å‡º (å« Token)")
        st.code(_limit_words(artifacts.generated_text or "(æ¨¡å‹ç”Ÿæˆä¸ºç©º)", 200))
        token_df = build_token_dataframe(artifacts)
        st.dataframe(token_df, width="stretch", hide_index=True)
        st.caption("æç¤ºï¼šæ¦‚ç‡è¶Šé«˜çš„ Token è¶Šç¡®å®šï¼Œå›°æƒ‘åº¦å±•ç¤ºæ¨¡å‹ä¸ç¡®å®šçš„èŠ‚ç‚¹ã€‚")

    with tabs[1]:
        if "æ³¨æ„åŠ›æƒé‡" in config["viz_dims"]:
            st.markdown("### æ³¨æ„åŠ›å±‚çº§")
            available_layers = config["settings"].attention_layers
            visible_layers = available_layers
            if len(available_layers) > 3:
                max_start = len(available_layers) - 3
                start_idx = st.slider(
                    "é€‰æ‹©æ³¨æ„åŠ›å±‚çª—å£ï¼ˆä¸€æ¬¡æœ€å¤šå±•ç¤º 3 å±‚ï¼‰",
                    min_value=0,
                    max_value=max_start,
                    value=st.session_state.get("attention_layer_window", 0),
                    key="attention-layer-window-slider",
                )
                st.session_state["attention_layer_window"] = start_idx
                visible_layers = available_layers[start_idx : start_idx + 3]
                st.caption(f"å½“å‰å±•ç¤ºå±‚ï¼š{visible_layers} Â· å…±é€‰æ‹© {len(available_layers)} å±‚ï¼Œå¯æ‹–åŠ¨æ»‘å—åˆ‡æ¢ã€‚")
            else:
                st.caption(f"å½“å‰å±•ç¤ºå±‚ï¼š{visible_layers}")

            token_count = len(artifacts.tokens)
            highlight_default = min(st.session_state.get("selected_token") or 0, max(token_count - 1, 0))
            if token_count > 1:
                highlight_idx = st.slider(
                    "é€‰æ‹©é«˜äº® Token ç´¢å¼•",
                    min_value=0,
                    max_value=token_count - 1,
                    value=highlight_default,
                    key="token-highlight-slider",
                )
                st.session_state["selected_token"] = highlight_idx
                st.caption(f"å½“å‰ Token #{highlight_idx}: `{artifacts.tokens[highlight_idx]}`")
            else:
                st.session_state["selected_token"] = None
            attention_fig, summaries = build_attention_figure(
                artifacts,
                layers=visible_layers,
                heads=config["settings"].attention_heads,
                theme=config["theme"],
                selected_token=st.session_state.get("selected_token"),
            )
            st.plotly_chart(attention_fig, width="stretch")
            st.markdown("\n".join([f"- {summary}" for summary in summaries]))
            _render_download_row(attention_fig, prefix="attention")
            st.caption("åº•å±‚å±‚å…³æ³¨è¯­æ³•é‚»è¿‘ï¼Œé«˜å±‚å±‚èšç„¦æŠ½è±¡è¯­ä¹‰ã€‚é€šè¿‡ä¸Šæ–¹æ»‘å—æŒ‘é€‰ Tokenï¼Œå¯è·¨å±‚è¿½è¸ªå…¶æ³¨æ„åŠ›ã€‚")
        else:
            st.warning("å·²å…³é—­æ³¨æ„åŠ›è§†å›¾ï¼Œå¯åœ¨ä¾§è¾¹æ é‡æ–°å¯ç”¨ã€‚")

    with tabs[2]:
        if "Token æ¨ç†æµ" in config["viz_dims"]:
            st.markdown("### æ¨ç†æµç¨‹")
            flow_fig, flow_df = build_token_flow_chart(artifacts, theme=config["theme"])
            st.plotly_chart(flow_fig, width="stretch")
            st.dataframe(flow_df, width="stretch", hide_index=True)
            _render_download_row(flow_fig, prefix="token_flow")
            st.caption("çº¢è‰²æ ‡è®°è¡¨ç¤ºæ¨¡å‹é«˜åº¦è‡ªä¿¡ï¼Œé»„è‰²æ„å‘³ç€çŠ¹è±«èŠ‚ç‚¹ï¼Œå¯ç”¨æ¥è§£é‡Šç”ŸæˆèŠ‚å¥ã€‚")
        else:
            st.warning("å·²å…³é—­ Token æ¨ç†æµè§†å›¾ã€‚")

    with tabs[3]:
        if "è¯­ä¹‰èšç±»" in config["viz_dims"]:
            st.markdown("### è¯­ä¹‰èšç±»")
            cluster_fig, cluster_df = build_semantic_cluster_figure(
                artifacts,
                layers=config["settings"].attention_layers,
                method=config["embed_method"],
                theme=config["theme"],
            )
            st.plotly_chart(cluster_fig, width="stretch")
            st.dataframe(cluster_df.head(100), width="stretch", hide_index=True)
            _render_download_row(cluster_fig, prefix="semantic")
            st.caption("æ•£ç‚¹ä½ç½®å±•ç¤º Token åœ¨è¯­ä¹‰ç©ºé—´çš„æŠ•å½±ï¼Œç›¸åŒé¢œè‰²è¡¨ç¤ºè¯­ä¹‰ç±»åˆ«ã€‚")
        else:
            st.warning("å·²å…³é—­è¯­ä¹‰èšç±»è§†å›¾ã€‚")


def _render_download_row(fig, prefix: str) -> None:
    """Render HTML and PNG download buttons for a figure."""

    col_html, col_png = st.columns(2)
    with col_html:
        st.download_button(
            "å¯¼å‡º HTML",
            data=figure_to_html_bytes(fig),
            file_name=f"{prefix}.html",
            mime="text/html",
            width="stretch",
        )
    with col_png:
        st.download_button(
            "å¯¼å‡º PNG",
            data=figure_to_png_bytes(fig),
            file_name=f"{prefix}.png",
            mime="image/png",
            width="stretch",
        )


def main() -> None:
    """Entrypoint for Streamlit."""

    st.set_page_config(page_title="GPT-2 å¯è§†åŒ–å·¥ä½œå°", page_icon="ğŸ§ ", layout="wide")
    _init_session_state()
    _show_guide_modal()
    config = _render_sidebar()
    _apply_theme_styles(config["theme"])
    st.title("GPT-2 å¯è§†åŒ–å·¥ä½œå°")
    st.caption("é¢å‘éä¸“ä¸šç”¨æˆ·çš„å±‚çº§æ€ç»´é€è§†â€”â€”åœ¨ CPU ä¸Šä¹Ÿèƒ½è¿è¡Œçš„å¯è§†åŒ–å·¥å…·ã€‚")
    st.markdown(f"å½“å‰æ¨¡å‹ï¼š{describe_model(config['settings'].model_size)}")
    _render_prompt_area()
    _handle_actions(config)
    st.divider()
    _render_results(config)


if __name__ == "__main__":
    main()
