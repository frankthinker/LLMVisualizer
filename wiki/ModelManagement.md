# 模型与缓存管理

## 支持的模型
| 代号 | Hugging Face ID | 参数量 | 层数/头数 | 上下文 | 适用场景 |
| ---- | --------------- | ------ | -------- | ------ | -------- |
| small | gpt2 | 124M | 12 / 12 | 1024 | CPU 体验、快速演示 |
| medium | gpt2-medium | 355M | 24 / 16 | 1024 | 推荐默认，兼顾速度与质量 |
| large | gpt2-large | 774M | 36 / 20 | 1024 | 需更多内存，适合分析性任务 |
| xl | gpt2-xl | 1.5B | 48 / 25 | 1024 | 需充足内存/显存，语义最稳 |

> 建议在硬件条件允许时使用 `medium` 及以上模型，以获得更好的注意力和语义聚类效果。

## 模型切换
- 侧边栏的“GPT-2 版本”选择器会触发模型下载与加载；首次使用大模型可能需要较长时间。
- 每次切换模型时，应用会自动清理之前缓存的模型（包括 GPU 显存），避免内存膨胀。
- “上下文窗口 (tokens)” 的默认值会根据模型上限设置，仍可手动调节以控制截断长度。

## 缓存目录
- 默认缓存目录：`~/.cache/huggingface/hub`。可通过侧边栏按钮在 Finder/Explorer 打开此目录。
- 如果希望迁移缓存，可以设置 `HF_HOME` 或 `TRANSFORMERS_CACHE` 环境变量，再启动应用。

## 清理缓存
- 点击“清理内存中的模型”会删除运行时缓存并释放显存/RAM；磁盘缓存仍保留，下一次推理会从本地重新加载。
- 如需彻底删除磁盘缓存，手动移除 `~/.cache/huggingface/hub/models--gpt2*` 即可。
